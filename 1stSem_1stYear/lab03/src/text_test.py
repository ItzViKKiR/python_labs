from text import *

print('normilize')
print(r'–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t  Out: ', normalize('–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t'))
print(r'—ë–∂–∏–∫, –Å–ª–∫–∞  Out: ', normalize('—ë–∂–∏–∫, –Å–ª–∫–∞', yo2e=True))
print(r'Hello\r\nWorld  Out: ', normalize('Hello\r\nWorld'))
print(r' –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã   Out: ', normalize(' –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã '))

print('tokenize')
print(r'–ø—Ä–∏–≤–µ—Ç –º–∏—Ä Out:', tokenize('–ø—Ä–∏–≤–µ—Ç –º–∏—Ä'))
print(r'hello,world!!! Out:', tokenize('hello,world!!!'))
print(r'–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ Out:', tokenize('–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ'))
print(r'2025 –≥–æ–¥ Out:', tokenize('2025 –≥–æ–¥'))
print(r'emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ Out:', tokenize('emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ'))

print('count_freq & top_n')
print(f'["a","b","a","c","b","a"] Out counter: {count_freq(["a","b","a","c","b","a"])} Out top_n {top_n(count_freq(["a","b","a","c","b","a"]))}')
print(f'["bb","aa","bb","aa","cc"] Out counter: {count_freq(["bb","aa","bb","aa","cc"])} Out top_n {top_n(count_freq(["bb","aa","bb","aa","cc"]), 2)}')
